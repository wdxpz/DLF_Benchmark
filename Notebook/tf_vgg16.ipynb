{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.examples.tutorials'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cff7180f26b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhelper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload_and_extract_cifar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhelper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCifar10Loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tf/notebooks/Notebook/helper.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.examples.tutorials'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')  \n",
    "from helper import download_and_extract_cifar\n",
    "from helper import Cifar10Loader\n",
    "\n",
    "dest = './dataset/cifar-10-batches-py/'\n",
    "cifar = Cifar10Loader(dest, normalize=True, \n",
    "                      zero_center=True,\n",
    "                      channel_mean_center=False)\n",
    "cifar.num_train\n",
    "\n",
    "X, y = cifar.load_test()\n",
    "half = cifar.num_test // 2\n",
    "X_test, X_valid = X[:half], X[half:]\n",
    "y_test, y_valid = y[:half], y[half:]\n",
    "\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0626 08:51:55.130888 140686170728256 deprecation.py:323] From <ipython-input-3-c2c630f59e0a>:232: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch: 0200 | Cost: 5.951\n",
      "Minibatch: 0400 | Cost: 3.150\n",
      "Minibatch: 0600 | Cost: 3.434\n",
      "Minibatch: 0800 | Cost: 2.238\n",
      "Minibatch: 1000 | Cost: 2.248\n",
      "Minibatch: 1200 | Cost: 2.357\n",
      "Minibatch: 1400 | Cost: 2.196\n",
      "Epoch: 001 | AvgCost: 2030.601 | Train/Valid ACC: 0.158/0.153\n",
      "Time elapsed: 0.71 min\n",
      "Minibatch: 0200 | Cost: 2.159\n",
      "Minibatch: 0400 | Cost: 1.976\n",
      "Minibatch: 0600 | Cost: 2.197\n",
      "Minibatch: 0800 | Cost: 2.402\n",
      "Minibatch: 1000 | Cost: 2.102\n",
      "Minibatch: 1200 | Cost: 2.170\n",
      "Minibatch: 1400 | Cost: 2.279\n",
      "Epoch: 002 | AvgCost: 2.183 | Train/Valid ACC: 0.198/0.188\n",
      "Time elapsed: 1.40 min\n",
      "Minibatch: 0200 | Cost: 2.087\n",
      "Minibatch: 0400 | Cost: 1.952\n",
      "Minibatch: 0600 | Cost: 2.222\n",
      "Minibatch: 0800 | Cost: 2.059\n",
      "Minibatch: 1000 | Cost: 1.817\n",
      "Minibatch: 1200 | Cost: 1.994\n",
      "Minibatch: 1400 | Cost: 2.111\n",
      "Epoch: 003 | AvgCost: 2.100 | Train/Valid ACC: 0.229/0.229\n",
      "Time elapsed: 2.10 min\n",
      "Minibatch: 0200 | Cost: 1.949\n",
      "Minibatch: 0400 | Cost: 2.018\n",
      "Minibatch: 0600 | Cost: 1.961\n",
      "Minibatch: 0800 | Cost: 2.028\n",
      "Minibatch: 1000 | Cost: 1.992\n",
      "Minibatch: 1200 | Cost: 1.728\n",
      "Minibatch: 1400 | Cost: 1.956\n",
      "Epoch: 004 | AvgCost: 1.974 | Train/Valid ACC: 0.240/0.240\n",
      "Time elapsed: 2.79 min\n",
      "Minibatch: 0200 | Cost: 1.837\n",
      "Minibatch: 0400 | Cost: 2.156\n",
      "Minibatch: 0600 | Cost: 1.971\n",
      "Minibatch: 0800 | Cost: 1.845\n",
      "Minibatch: 1000 | Cost: 1.852\n",
      "Minibatch: 1200 | Cost: 1.849\n",
      "Minibatch: 1400 | Cost: 1.661\n",
      "Epoch: 005 | AvgCost: 1.893 | Train/Valid ACC: 0.274/0.272\n",
      "Time elapsed: 3.48 min\n",
      "Minibatch: 0200 | Cost: 1.766\n",
      "Minibatch: 0400 | Cost: 1.551\n",
      "Minibatch: 0600 | Cost: 1.665\n",
      "Minibatch: 0800 | Cost: 1.852\n",
      "Minibatch: 1000 | Cost: 1.690\n",
      "Minibatch: 1200 | Cost: 1.648\n",
      "Minibatch: 1400 | Cost: 1.709\n",
      "Epoch: 006 | AvgCost: 1.819 | Train/Valid ACC: 0.306/0.302\n",
      "Time elapsed: 4.17 min\n",
      "Minibatch: 0200 | Cost: 1.629\n",
      "Minibatch: 0400 | Cost: 1.772\n",
      "Minibatch: 0600 | Cost: 1.517\n",
      "Minibatch: 0800 | Cost: 1.679\n",
      "Minibatch: 1000 | Cost: 1.758\n",
      "Minibatch: 1200 | Cost: 1.790\n",
      "Minibatch: 1400 | Cost: 1.675\n",
      "Epoch: 007 | AvgCost: 1.727 | Train/Valid ACC: 0.338/0.329\n",
      "Time elapsed: 4.87 min\n",
      "Minibatch: 0200 | Cost: 1.578\n",
      "Minibatch: 0400 | Cost: 1.714\n",
      "Minibatch: 0600 | Cost: 1.522\n",
      "Minibatch: 0800 | Cost: 1.529\n",
      "Minibatch: 1000 | Cost: 1.622\n",
      "Minibatch: 1200 | Cost: 1.826\n",
      "Minibatch: 1400 | Cost: 1.720\n",
      "Epoch: 008 | AvgCost: 1.666 | Train/Valid ACC: 0.375/0.362\n",
      "Time elapsed: 5.56 min\n",
      "Minibatch: 0200 | Cost: 1.593\n",
      "Minibatch: 0400 | Cost: 1.574\n",
      "Minibatch: 0600 | Cost: 1.455\n",
      "Minibatch: 0800 | Cost: 1.205\n",
      "Minibatch: 1000 | Cost: 1.517\n",
      "Minibatch: 1200 | Cost: 1.605\n",
      "Minibatch: 1400 | Cost: 1.548\n",
      "Epoch: 009 | AvgCost: 1.578 | Train/Valid ACC: 0.424/0.414\n",
      "Time elapsed: 6.25 min\n",
      "Minibatch: 0200 | Cost: 1.411\n",
      "Minibatch: 0400 | Cost: 1.159\n",
      "Minibatch: 0600 | Cost: 1.606\n",
      "Minibatch: 0800 | Cost: 1.400\n",
      "Minibatch: 1000 | Cost: 1.454\n",
      "Minibatch: 1200 | Cost: 1.488\n",
      "Minibatch: 1400 | Cost: 1.637\n",
      "Epoch: 010 | AvgCost: 1.446 | Train/Valid ACC: 0.487/0.467\n",
      "Time elapsed: 6.95 min\n",
      "Minibatch: 0200 | Cost: 1.569\n",
      "Minibatch: 0400 | Cost: 1.372\n",
      "Minibatch: 0600 | Cost: 1.557\n",
      "Minibatch: 0800 | Cost: 1.193\n",
      "Minibatch: 1000 | Cost: 1.424\n",
      "Minibatch: 1200 | Cost: 1.578\n",
      "Minibatch: 1400 | Cost: 1.414\n",
      "Epoch: 011 | AvgCost: 1.316 | Train/Valid ACC: 0.581/0.552\n",
      "Time elapsed: 7.64 min\n",
      "Minibatch: 0200 | Cost: 1.386\n",
      "Minibatch: 0400 | Cost: 1.171\n",
      "Minibatch: 0600 | Cost: 1.003\n",
      "Minibatch: 0800 | Cost: 1.256\n",
      "Minibatch: 1000 | Cost: 1.187\n",
      "Minibatch: 1200 | Cost: 1.127\n",
      "Minibatch: 1400 | Cost: 0.855\n",
      "Epoch: 012 | AvgCost: 1.190 | Train/Valid ACC: 0.612/0.595\n",
      "Time elapsed: 8.33 min\n",
      "Minibatch: 0200 | Cost: 0.898\n",
      "Minibatch: 0400 | Cost: 1.236\n",
      "Minibatch: 0600 | Cost: 1.227\n",
      "Minibatch: 0800 | Cost: 1.143\n",
      "Minibatch: 1000 | Cost: 1.055\n",
      "Minibatch: 1200 | Cost: 1.065\n",
      "Minibatch: 1400 | Cost: 1.266\n",
      "Epoch: 013 | AvgCost: 1.095 | Train/Valid ACC: 0.652/0.629\n",
      "Time elapsed: 9.03 min\n",
      "Minibatch: 0200 | Cost: 1.241\n",
      "Minibatch: 0400 | Cost: 0.958\n",
      "Minibatch: 0600 | Cost: 0.944\n",
      "Minibatch: 0800 | Cost: 0.811\n",
      "Minibatch: 1000 | Cost: 1.001\n",
      "Minibatch: 1200 | Cost: 0.754\n",
      "Minibatch: 1400 | Cost: 1.316\n",
      "Epoch: 014 | AvgCost: 1.006 | Train/Valid ACC: 0.663/0.629\n",
      "Time elapsed: 9.72 min\n",
      "Minibatch: 0200 | Cost: 1.037\n",
      "Minibatch: 0400 | Cost: 1.077\n",
      "Minibatch: 0600 | Cost: 1.162\n",
      "Minibatch: 0800 | Cost: 0.902\n",
      "Minibatch: 1000 | Cost: 0.490\n",
      "Minibatch: 1200 | Cost: 0.688\n",
      "Minibatch: 1400 | Cost: 0.676\n",
      "Epoch: 015 | AvgCost: 0.927 | Train/Valid ACC: 0.721/0.682\n",
      "Time elapsed: 10.41 min\n",
      "Minibatch: 0200 | Cost: 0.371\n",
      "Minibatch: 0400 | Cost: 0.728\n",
      "Minibatch: 0600 | Cost: 0.916\n",
      "Minibatch: 0800 | Cost: 0.990\n",
      "Minibatch: 1000 | Cost: 1.038\n",
      "Minibatch: 1200 | Cost: 0.799\n",
      "Minibatch: 1400 | Cost: 0.710\n",
      "Epoch: 016 | AvgCost: 0.855 | Train/Valid ACC: 0.740/0.682\n",
      "Time elapsed: 11.10 min\n",
      "Minibatch: 0200 | Cost: 0.671\n",
      "Minibatch: 0400 | Cost: 0.526\n",
      "Minibatch: 0600 | Cost: 0.408\n",
      "Minibatch: 0800 | Cost: 0.912\n",
      "Minibatch: 1000 | Cost: 0.705\n",
      "Minibatch: 1200 | Cost: 0.665\n",
      "Minibatch: 1400 | Cost: 0.501\n",
      "Epoch: 017 | AvgCost: 0.784 | Train/Valid ACC: 0.755/0.693\n",
      "Time elapsed: 11.79 min\n",
      "Minibatch: 0200 | Cost: 0.902\n",
      "Minibatch: 0400 | Cost: 0.788\n",
      "Minibatch: 0600 | Cost: 0.814\n",
      "Minibatch: 0800 | Cost: 0.858\n",
      "Minibatch: 1000 | Cost: 0.687\n",
      "Minibatch: 1200 | Cost: 0.658\n",
      "Minibatch: 1400 | Cost: 0.800\n",
      "Epoch: 018 | AvgCost: 0.736 | Train/Valid ACC: 0.720/0.669\n",
      "Time elapsed: 12.48 min\n",
      "Minibatch: 0200 | Cost: 0.713\n",
      "Minibatch: 0400 | Cost: 0.705\n",
      "Minibatch: 0600 | Cost: 0.470\n",
      "Minibatch: 0800 | Cost: 0.524\n",
      "Minibatch: 1000 | Cost: 0.868\n",
      "Minibatch: 1200 | Cost: 0.691\n",
      "Minibatch: 1400 | Cost: 0.625\n",
      "Epoch: 019 | AvgCost: 0.699 | Train/Valid ACC: 0.813/0.732\n",
      "Time elapsed: 13.17 min\n",
      "Minibatch: 0200 | Cost: 0.641\n",
      "Minibatch: 0400 | Cost: 0.844\n",
      "Minibatch: 0600 | Cost: 0.703\n",
      "Minibatch: 0800 | Cost: 0.649\n",
      "Minibatch: 1000 | Cost: 0.667\n",
      "Minibatch: 1200 | Cost: 0.670\n",
      "Minibatch: 1400 | Cost: 0.424\n",
      "Epoch: 020 | AvgCost: 0.666 | Train/Valid ACC: 0.809/0.728\n",
      "Time elapsed: 13.86 min\n",
      "Minibatch: 0200 | Cost: 0.822\n",
      "Minibatch: 0400 | Cost: 1.881\n",
      "Minibatch: 0600 | Cost: 0.618\n",
      "Minibatch: 0800 | Cost: 0.711\n",
      "Minibatch: 1000 | Cost: 0.819\n",
      "Minibatch: 1200 | Cost: 0.381\n",
      "Minibatch: 1400 | Cost: 0.548\n",
      "Epoch: 021 | AvgCost: 0.634 | Train/Valid ACC: 0.824/0.724\n",
      "Time elapsed: 14.55 min\n",
      "Minibatch: 0200 | Cost: 0.954\n",
      "Minibatch: 0400 | Cost: 0.620\n",
      "Minibatch: 0600 | Cost: 0.577\n",
      "Minibatch: 0800 | Cost: 0.576\n",
      "Minibatch: 1000 | Cost: 0.623\n",
      "Minibatch: 1200 | Cost: 0.686\n",
      "Minibatch: 1400 | Cost: 0.714\n",
      "Epoch: 022 | AvgCost: 0.646 | Train/Valid ACC: 0.831/0.743\n",
      "Time elapsed: 15.24 min\n",
      "Minibatch: 0200 | Cost: 0.740\n",
      "Minibatch: 0400 | Cost: 0.540\n",
      "Minibatch: 0600 | Cost: 0.298\n",
      "Minibatch: 0800 | Cost: 0.442\n",
      "Minibatch: 1000 | Cost: 0.563\n",
      "Minibatch: 1200 | Cost: 0.669\n",
      "Minibatch: 1400 | Cost: 0.549\n",
      "Epoch: 023 | AvgCost: 0.581 | Train/Valid ACC: 0.846/0.741\n",
      "Time elapsed: 15.93 min\n",
      "Minibatch: 0200 | Cost: 0.554\n",
      "Minibatch: 0400 | Cost: 0.728\n",
      "Minibatch: 0600 | Cost: 0.189\n",
      "Minibatch: 0800 | Cost: 0.242\n",
      "Minibatch: 1000 | Cost: 0.159\n",
      "Minibatch: 1200 | Cost: 0.512\n",
      "Minibatch: 1400 | Cost: 0.574\n",
      "Epoch: 024 | AvgCost: 0.488 | Train/Valid ACC: 0.853/0.739\n",
      "Time elapsed: 16.62 min\n",
      "Minibatch: 0200 | Cost: 0.391\n",
      "Minibatch: 0400 | Cost: 0.517\n",
      "Minibatch: 0600 | Cost: 0.177\n",
      "Minibatch: 0800 | Cost: 0.269\n",
      "Minibatch: 1000 | Cost: 0.279\n",
      "Minibatch: 1200 | Cost: 0.449\n",
      "Minibatch: 1400 | Cost: 0.490\n",
      "Epoch: 025 | AvgCost: 0.547 | Train/Valid ACC: 0.738/0.677\n",
      "Time elapsed: 17.31 min\n",
      "Minibatch: 0200 | Cost: 0.345\n",
      "Minibatch: 0400 | Cost: 1.270\n",
      "Minibatch: 0600 | Cost: 0.709\n",
      "Minibatch: 0800 | Cost: 0.621\n",
      "Minibatch: 1000 | Cost: 0.542\n",
      "Minibatch: 1200 | Cost: 1.032\n",
      "Minibatch: 1400 | Cost: 0.523\n",
      "Epoch: 026 | AvgCost: 0.786 | Train/Valid ACC: 0.856/0.747\n",
      "Time elapsed: 18.00 min\n",
      "Minibatch: 0200 | Cost: 0.437\n",
      "Minibatch: 0400 | Cost: 0.813\n",
      "Minibatch: 0600 | Cost: 0.398\n",
      "Minibatch: 0800 | Cost: 0.397\n",
      "Minibatch: 1000 | Cost: 0.549\n",
      "Minibatch: 1200 | Cost: 0.392\n",
      "Minibatch: 1400 | Cost: 0.484\n",
      "Epoch: 027 | AvgCost: 0.496 | Train/Valid ACC: 0.881/0.760\n",
      "Time elapsed: 18.69 min\n",
      "Minibatch: 0200 | Cost: 0.416\n",
      "Minibatch: 0400 | Cost: 0.237\n",
      "Minibatch: 0600 | Cost: 0.368\n",
      "Minibatch: 0800 | Cost: 0.517\n",
      "Minibatch: 1000 | Cost: 0.574\n",
      "Minibatch: 1200 | Cost: 0.325\n",
      "Minibatch: 1400 | Cost: 0.477\n",
      "Epoch: 028 | AvgCost: 0.453 | Train/Valid ACC: 0.844/0.740\n",
      "Time elapsed: 19.39 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch: 0200 | Cost: 0.973\n",
      "Minibatch: 0400 | Cost: 0.450\n",
      "Minibatch: 0600 | Cost: 0.361\n",
      "Minibatch: 0800 | Cost: 0.301\n",
      "Minibatch: 1000 | Cost: 0.372\n",
      "Minibatch: 1200 | Cost: 0.426\n",
      "Minibatch: 1400 | Cost: 0.305\n",
      "Epoch: 029 | AvgCost: 0.435 | Train/Valid ACC: 0.895/0.762\n",
      "Time elapsed: 20.08 min\n",
      "Minibatch: 0200 | Cost: 0.202\n",
      "Minibatch: 0400 | Cost: 0.203\n",
      "Minibatch: 0600 | Cost: 0.705\n",
      "Minibatch: 0800 | Cost: 1.153\n",
      "Minibatch: 1000 | Cost: 0.467\n",
      "Minibatch: 1200 | Cost: 0.606\n",
      "Minibatch: 1400 | Cost: 0.510\n",
      "Epoch: 030 | AvgCost: 0.554 | Train/Valid ACC: 0.851/0.739\n",
      "Time elapsed: 20.77 min\n",
      "Total Training Time: 20.77 min\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "# Other\n",
    "print_interval = 200\n",
    "\n",
    "# Architecture\n",
    "image_width, image_height, image_depth = 32, 32, 3\n",
    "n_classes = 10\n",
    "\n",
    "\n",
    "##########################\n",
    "### WRAPPER FUNCTIONS\n",
    "##########################\n",
    "\n",
    "def conv_layer(input, input_channels, output_channels, \n",
    "               kernel_size, strides, scope, padding='SAME'):\n",
    "    with tf.name_scope(scope):\n",
    "        weights_shape = kernel_size + [input_channels, output_channels]\n",
    "        weights = tf.Variable(tf.truncated_normal(shape=weights_shape,\n",
    "                                                  mean=0.0,\n",
    "                                                  stddev=0.1,\n",
    "                                                  dtype=tf.float32),\n",
    "                                                  name='weights')\n",
    "        biases = tf.Variable(tf.zeros(shape=[output_channels]),\n",
    "                             name='biases')\n",
    "        conv = tf.nn.conv2d(input=input,\n",
    "                            filter=weights,\n",
    "                            strides=strides,\n",
    "                            padding=padding,\n",
    "                            name='convolution')\n",
    "        out = tf.nn.bias_add(conv, biases, name='logits')\n",
    "        out = tf.nn.relu(out, name='activation')\n",
    "        return out\n",
    "\n",
    "\n",
    "def fc_layer(input, output_nodes, scope,\n",
    "             activation=None, seed=None):\n",
    "    with tf.name_scope(scope):\n",
    "        shape = int(np.prod(input.get_shape()[1:]))\n",
    "        flat_input = tf.reshape(input, [-1, shape])\n",
    "        weights = tf.Variable(tf.truncated_normal(shape=[shape,\n",
    "                                                         output_nodes],\n",
    "                                                  mean=0.0,\n",
    "                                                  stddev=0.1,\n",
    "                                                  dtype=tf.float32,\n",
    "                                                  seed=seed),\n",
    "                                                  name='weights')\n",
    "        biases = tf.Variable(tf.zeros(shape=[output_nodes]),\n",
    "                             name='biases')\n",
    "        act = tf.nn.bias_add(tf.matmul(flat_input, weights), biases, \n",
    "                             name='logits')\n",
    "\n",
    "        if activation is not None:\n",
    "            act = activation(act, name='activation')\n",
    "\n",
    "        return act\n",
    "\n",
    "\n",
    "##########################\n",
    "### GRAPH DEFINITION\n",
    "##########################\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "\n",
    "    # Input data\n",
    "    tf_x = tf.placeholder(tf.float32, [None, image_width, image_height, image_depth], name='features')\n",
    "    tf_y = tf.placeholder(tf.float32, [None, n_classes], name='targets')\n",
    "     \n",
    "    ##########################\n",
    "    ### VGG16 Model\n",
    "    ##########################\n",
    "\n",
    "    # =========\n",
    "    # BLOCK 1\n",
    "    # =========\n",
    "    conv_layer_1 = conv_layer(input=tf_x,\n",
    "                              input_channels=3,\n",
    "                              output_channels=64,\n",
    "                              kernel_size=[3, 3],\n",
    "                              strides=[1, 1, 1, 1],\n",
    "                              scope='conv1')\n",
    "    \n",
    "    conv_layer_2 = conv_layer(input=conv_layer_1,\n",
    "                              input_channels=64,\n",
    "                              output_channels=64,\n",
    "                              kernel_size=[3, 3],\n",
    "                              strides=[1, 1, 1, 1],\n",
    "                              scope='conv2')    \n",
    "    \n",
    "    pool_layer_1 = tf.nn.max_pool(conv_layer_2,\n",
    "                                  ksize=[1, 2, 2, 1], \n",
    "                                  strides=[1, 2, 2, 1],\n",
    "                                  padding='SAME',\n",
    "                                  name='pool1') \n",
    "    # =========\n",
    "    # BLOCK 2\n",
    "    # =========\n",
    "    conv_layer_3 = conv_layer(input=pool_layer_1,\n",
    "                              input_channels=64,\n",
    "                              output_channels=128,\n",
    "                              kernel_size=[3, 3],\n",
    "                              strides=[1, 1, 1, 1],\n",
    "                              scope='conv3')    \n",
    "    \n",
    "    conv_layer_4 = conv_layer(input=conv_layer_3,\n",
    "                              input_channels=128,\n",
    "                              output_channels=128,\n",
    "                              kernel_size=[3, 3],\n",
    "                              strides=[1, 1, 1, 1],\n",
    "                              scope='conv4')    \n",
    "    \n",
    "    pool_layer_2 = tf.nn.max_pool(conv_layer_4,\n",
    "                                  ksize=[1, 2, 2, 1], \n",
    "                                  strides=[1, 2, 2, 1],\n",
    "                                  padding='SAME',\n",
    "                                  name='pool2') \n",
    "    # =========\n",
    "    # BLOCK 3\n",
    "    # =========\n",
    "    conv_layer_5 = conv_layer(input=pool_layer_2,\n",
    "                              input_channels=128,\n",
    "                              output_channels=256,\n",
    "                              kernel_size=[3, 3],\n",
    "                              strides=[1, 1, 1, 1],\n",
    "                              scope='conv5')        \n",
    "    \n",
    "    conv_layer_6 = conv_layer(input=conv_layer_5,\n",
    "                              input_channels=256,\n",
    "                              output_channels=256,\n",
    "                              kernel_size=[3, 3],\n",
    "                              strides=[1, 1, 1, 1],\n",
    "                              scope='conv6')      \n",
    "    \n",
    "    conv_layer_7 = conv_layer(input=conv_layer_6,\n",
    "                              input_channels=256,\n",
    "                              output_channels=256,\n",
    "                              kernel_size=[3, 3],\n",
    "                              strides=[1, 1, 1, 1],\n",
    "                              scope='conv7')\n",
    "    \n",
    "    pool_layer_3 = tf.nn.max_pool(conv_layer_7,\n",
    "                                  ksize=[1, 2, 2, 1], \n",
    "                                  strides=[1, 2, 2, 1],\n",
    "                                  padding='SAME',\n",
    "                                  name='pool3') \n",
    "    # =========\n",
    "    # BLOCK 4\n",
    "    # =========\n",
    "    conv_layer_8 = conv_layer(input=pool_layer_3,\n",
    "                              input_channels=256,\n",
    "                              output_channels=512,\n",
    "                              kernel_size=[3, 3],\n",
    "                              strides=[1, 1, 1, 1],\n",
    "                              scope='conv8')      \n",
    "    \n",
    "    conv_layer_9 = conv_layer(input=conv_layer_8,\n",
    "                              input_channels=512,\n",
    "                              output_channels=512,\n",
    "                              kernel_size=[3, 3],\n",
    "                              strides=[1, 1, 1, 1],\n",
    "                              scope='conv9')     \n",
    "    \n",
    "    conv_layer_10 = conv_layer(input=conv_layer_9,\n",
    "                               input_channels=512,\n",
    "                               output_channels=512,\n",
    "                               kernel_size=[3, 3],\n",
    "                               strides=[1, 1, 1, 1],\n",
    "                               scope='conv10')   \n",
    "    \n",
    "    pool_layer_4 = tf.nn.max_pool(conv_layer_10,\n",
    "                                  ksize=[1, 2, 2, 1], \n",
    "                                  strides=[1, 2, 2, 1],\n",
    "                                  padding='SAME',\n",
    "                                  name='pool4') \n",
    "    # =========\n",
    "    # BLOCK 5\n",
    "    # =========\n",
    "    conv_layer_11 = conv_layer(input=pool_layer_4,\n",
    "                               input_channels=512,\n",
    "                               output_channels=512,\n",
    "                               kernel_size=[3, 3],\n",
    "                               strides=[1, 1, 1, 1],\n",
    "                               scope='conv11')   \n",
    "    \n",
    "    conv_layer_12 = conv_layer(input=conv_layer_11,\n",
    "                               input_channels=512,\n",
    "                               output_channels=512,\n",
    "                               kernel_size=[3, 3],\n",
    "                               strides=[1, 1, 1, 1],\n",
    "                               scope='conv12')   \n",
    "\n",
    "    conv_layer_13 = conv_layer(input=conv_layer_12,\n",
    "                               input_channels=512,\n",
    "                               output_channels=512,\n",
    "                               kernel_size=[3, 3],\n",
    "                               strides=[1, 1, 1, 1],\n",
    "                               scope='conv13') \n",
    "    \n",
    "    pool_layer_5 = tf.nn.max_pool(conv_layer_12,\n",
    "                                  ksize=[1, 2, 2, 1], \n",
    "                                  strides=[1, 2, 2, 1],\n",
    "                                  padding='SAME',\n",
    "                                  name='pool5')     \n",
    "    # ===========\n",
    "    # CLASSIFIER\n",
    "    # ===========\n",
    "    \n",
    "    fc_layer_1 = fc_layer(input=pool_layer_5, \n",
    "                          output_nodes=4096,\n",
    "                          activation=tf.nn.relu,\n",
    "                          scope='fc1')\n",
    "    \n",
    "    fc_layer_2 = fc_layer(input=fc_layer_1, \n",
    "                          output_nodes=4096,\n",
    "                          activation=tf.nn.relu,\n",
    "                          scope='fc2')\n",
    "\n",
    "    out_layer = fc_layer(input=fc_layer_2, \n",
    "                         output_nodes=n_classes,\n",
    "                         activation=None,\n",
    "                         scope='output_layer')\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=tf_y)\n",
    "    cost = tf.reduce_mean(loss, name='cost')\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train = optimizer.minimize(cost, name='train')\n",
    "\n",
    "    # Prediction\n",
    "    correct_prediction = tf.equal(tf.argmax(tf_y, 1), tf.argmax(out_layer, 1), \n",
    "                                  name='correct_predictions')\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "\n",
    "    # Saver to save session for reuse\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    \n",
    "##########################\n",
    "### TRAINING & EVALUATION\n",
    "##########################\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        avg_cost = 0.\n",
    "        mbatch_cnt = 0\n",
    "        for batch_x, batch_y in cifar.load_train_epoch(shuffle=True, batch_size=batch_size):\n",
    "            \n",
    "            mbatch_cnt += 1\n",
    "            _, c = sess.run(['train', 'cost:0'], feed_dict={'features:0': batch_x,\n",
    "                                                            'targets:0': batch_y})\n",
    "            avg_cost += c\n",
    "\n",
    "            if not mbatch_cnt % print_interval:\n",
    "                print(\"Minibatch: %04d | Cost: %.3f\" % (mbatch_cnt, c))\n",
    "                \n",
    "        # ===================\n",
    "        # Training Accuracy\n",
    "        # ===================\n",
    "        n_predictions, n_correct = 0, 0\n",
    "        for batch_x, batch_y in cifar.load_train_epoch(batch_size=batch_size):\n",
    "        \n",
    "            p = sess.run('correct_predictions:0', \n",
    "                         feed_dict={'features:0': batch_x,\n",
    "                                    'targets:0':  batch_y})\n",
    "            n_correct += np.sum(p)\n",
    "            n_predictions += p.shape[0]\n",
    "        train_acc = n_correct / n_predictions\n",
    "        \n",
    "        \n",
    "        # ===================\n",
    "        # Validation Accuracy\n",
    "        # ===================\n",
    "        #valid_acc = sess.run('accuracy:0', feed_dict={'features:0': X_valid,\n",
    "        #                                              'targets:0': y_valid})\n",
    "        # ---------------------------------------\n",
    "        # workaround for GPUs with <= 4 Gb memory\n",
    "        n_predictions, n_correct = 0, 0\n",
    "        indices = np.arange(y_valid.shape[0])\n",
    "        chunksize = 500\n",
    "        for start_idx in range(0, indices.shape[0] - chunksize + 1, chunksize):\n",
    "            index_slice = indices[start_idx:start_idx + chunksize]\n",
    "            p = sess.run('correct_predictions:0', \n",
    "                         feed_dict={'features:0': X_valid[index_slice],\n",
    "                                    'targets:0': y_valid[index_slice]})\n",
    "            n_correct += np.sum(p)\n",
    "            n_predictions += p.shape[0]\n",
    "        valid_acc = n_correct / n_predictions\n",
    "        # ---------------------------------------\n",
    "                                                \n",
    "        print(\"Epoch: %03d | AvgCost: %.3f\" % (epoch + 1, avg_cost / mbatch_cnt), end=\"\")\n",
    "        print(\" | Train/Valid ACC: %.3f/%.3f\" % (train_acc, valid_acc))\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "        \n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "    saver.save(sess, save_path='./model/convnet-vgg16.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0626 09:12:45.213761 140686170728256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ACC: 0.747\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### RELOAD & TEST\n",
    "##########################\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    saver.restore(sess, save_path='./model/convnet-vgg16.ckpt')\n",
    "    \n",
    "    # test_acc = sess.run('accuracy:0', feed_dict={'features:0': X_test,\n",
    "    #                                              'targets:0': y_test})\n",
    "    # ---------------------------------------\n",
    "    # workaround for GPUs with <= 4 Gb memory\n",
    "    n_predictions, n_correct = 0, 0\n",
    "    indices = np.arange(y_test.shape[0])\n",
    "    chunksize = 500\n",
    "    for start_idx in range(0, indices.shape[0] - chunksize + 1, chunksize):\n",
    "        index_slice = indices[start_idx:start_idx + chunksize]\n",
    "        p = sess.run('correct_predictions:0', \n",
    "                     feed_dict={'features:0': X_test[index_slice],\n",
    "                                'targets:0': y_test[index_slice]})\n",
    "        n_correct += np.sum(p)\n",
    "        n_predictions += p.shape[0]\n",
    "    test_acc = n_correct / n_predictions\n",
    "    # ---------------------------------------\n",
    "\n",
    "    print('Test ACC: %.3f' % test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
