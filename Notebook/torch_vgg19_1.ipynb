{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "# Hyperparameters\n",
    "random_seed = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "batch_size = 128\n",
    "\n",
    "# Architecture\n",
    "num_features = 784\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Image batch dimensions: torch.Size([128, 3, 32, 32])\n",
      "Image label dimensions: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### MNIST DATASET\n",
    "##########################\n",
    "\n",
    "# Note transforms.ToTensor() scales input images\n",
    "# to 0-1 range\n",
    "train_dataset = datasets.CIFAR10(root='dataset', \n",
    "                                 train=True, \n",
    "                                 transform=transforms.ToTensor(),\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='dataset', \n",
    "                                train=False, \n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         shuffle=False)\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "\n",
    "class VGG16(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(VGG16, self).__init__()\n",
    "        \n",
    "        # calculate same padding:\n",
    "        # (w - k + 2*p)/s + 1 = o\n",
    "        # => p = (s(o-1) - w + k)/2\n",
    "        \n",
    "        self.block_1 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=3,\n",
    "                          out_channels=64,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          # (1(32-1)- 32 + 3)/2 = 1\n",
    "                          padding=1), \n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=64,\n",
    "                          out_channels=64,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        self.block_2 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=64,\n",
    "                          out_channels=128,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=128,\n",
    "                          out_channels=128,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        self.block_3 = nn.Sequential(        \n",
    "                nn.Conv2d(in_channels=128,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),        \n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "          \n",
    "        self.block_4 = nn.Sequential(   \n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),        \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),        \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),   \n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        self.block_5 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),            \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),            \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),   \n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))             \n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "                nn.Linear(512, 4096),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(4096, num_classes)\n",
    "        )\n",
    "            \n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                #n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                #m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "                m.weight.detach().normal_(0, 0.05)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.detach().zero_()\n",
    "            elif isinstance(m, torch.nn.Linear):\n",
    "                m.weight.detach().normal_(0, 0.05)\n",
    "                m.bias.detach().detach().zero_()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.block_3(x)\n",
    "        x = self.block_4(x)\n",
    "        x = self.block_5(x)\n",
    "        logits = self.classifier(x.view(-1, 512))\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "\n",
    "        return logits, probas\n",
    "\n",
    "    \n",
    "torch.manual_seed(random_seed)\n",
    "model = VGG16(num_features=num_features,\n",
    "              num_classes=num_classes)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/030 | Batch 0000/0391 | Cost: 1061.4156\n",
      "Epoch: 001/030 | Batch 0050/0391 | Cost: 2.3033\n",
      "Epoch: 001/030 | Batch 0100/0391 | Cost: 2.2886\n",
      "Epoch: 001/030 | Batch 0150/0391 | Cost: 2.2971\n",
      "Epoch: 001/030 | Batch 0200/0391 | Cost: 2.0090\n",
      "Epoch: 001/030 | Batch 0250/0391 | Cost: 2.0277\n",
      "Epoch: 001/030 | Batch 0300/0391 | Cost: 2.0415\n",
      "Epoch: 001/030 | Batch 0350/0391 | Cost: 1.7686\n",
      "Epoch: 001/030 | Train: 31.120% | Loss: 1.783\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 002/030 | Batch 0000/0391 | Cost: 1.8740\n",
      "Epoch: 002/030 | Batch 0050/0391 | Cost: 1.7443\n",
      "Epoch: 002/030 | Batch 0100/0391 | Cost: 1.4892\n",
      "Epoch: 002/030 | Batch 0150/0391 | Cost: 1.5731\n",
      "Epoch: 002/030 | Batch 0200/0391 | Cost: 1.5534\n",
      "Epoch: 002/030 | Batch 0250/0391 | Cost: 1.4907\n",
      "Epoch: 002/030 | Batch 0300/0391 | Cost: 1.6997\n",
      "Epoch: 002/030 | Batch 0350/0391 | Cost: 1.6559\n",
      "Epoch: 002/030 | Train: 47.440% | Loss: 1.430\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 003/030 | Batch 0000/0391 | Cost: 1.5113\n",
      "Epoch: 003/030 | Batch 0050/0391 | Cost: 1.2819\n",
      "Epoch: 003/030 | Batch 0100/0391 | Cost: 1.3606\n",
      "Epoch: 003/030 | Batch 0150/0391 | Cost: 1.4638\n",
      "Epoch: 003/030 | Batch 0200/0391 | Cost: 1.5752\n",
      "Epoch: 003/030 | Batch 0250/0391 | Cost: 1.3480\n",
      "Epoch: 003/030 | Batch 0300/0391 | Cost: 1.4611\n",
      "Epoch: 003/030 | Batch 0350/0391 | Cost: 1.4123\n",
      "Epoch: 003/030 | Train: 51.668% | Loss: 1.321\n",
      "Time elapsed: 1.53 min\n",
      "Epoch: 004/030 | Batch 0000/0391 | Cost: 1.2862\n",
      "Epoch: 004/030 | Batch 0050/0391 | Cost: 1.2751\n",
      "Epoch: 004/030 | Batch 0100/0391 | Cost: 1.4208\n",
      "Epoch: 004/030 | Batch 0150/0391 | Cost: 1.1935\n",
      "Epoch: 004/030 | Batch 0200/0391 | Cost: 1.1903\n",
      "Epoch: 004/030 | Batch 0250/0391 | Cost: 1.1950\n",
      "Epoch: 004/030 | Batch 0300/0391 | Cost: 1.3443\n",
      "Epoch: 004/030 | Batch 0350/0391 | Cost: 1.2318\n",
      "Epoch: 004/030 | Train: 56.770% | Loss: 1.176\n",
      "Time elapsed: 2.03 min\n",
      "Epoch: 005/030 | Batch 0000/0391 | Cost: 1.1249\n",
      "Epoch: 005/030 | Batch 0050/0391 | Cost: 1.1677\n",
      "Epoch: 005/030 | Batch 0100/0391 | Cost: 0.9436\n",
      "Epoch: 005/030 | Batch 0150/0391 | Cost: 1.0558\n",
      "Epoch: 005/030 | Batch 0200/0391 | Cost: 0.9577\n",
      "Epoch: 005/030 | Batch 0250/0391 | Cost: 1.0916\n",
      "Epoch: 005/030 | Batch 0300/0391 | Cost: 1.0226\n",
      "Epoch: 005/030 | Batch 0350/0391 | Cost: 1.2384\n",
      "Epoch: 005/030 | Train: 61.302% | Loss: 1.067\n",
      "Time elapsed: 2.54 min\n",
      "Epoch: 006/030 | Batch 0000/0391 | Cost: 1.0241\n",
      "Epoch: 006/030 | Batch 0050/0391 | Cost: 1.0240\n",
      "Epoch: 006/030 | Batch 0100/0391 | Cost: 0.9237\n",
      "Epoch: 006/030 | Batch 0150/0391 | Cost: 1.0403\n",
      "Epoch: 006/030 | Batch 0200/0391 | Cost: 1.0931\n",
      "Epoch: 006/030 | Batch 0250/0391 | Cost: 0.9038\n",
      "Epoch: 006/030 | Batch 0300/0391 | Cost: 0.8586\n",
      "Epoch: 006/030 | Batch 0350/0391 | Cost: 1.1006\n",
      "Epoch: 006/030 | Train: 64.274% | Loss: 0.989\n",
      "Time elapsed: 3.05 min\n",
      "Epoch: 007/030 | Batch 0000/0391 | Cost: 1.1605\n",
      "Epoch: 007/030 | Batch 0050/0391 | Cost: 1.0157\n",
      "Epoch: 007/030 | Batch 0100/0391 | Cost: 0.9663\n",
      "Epoch: 007/030 | Batch 0150/0391 | Cost: 1.1401\n",
      "Epoch: 007/030 | Batch 0200/0391 | Cost: 0.8211\n",
      "Epoch: 007/030 | Batch 0250/0391 | Cost: 0.9102\n",
      "Epoch: 007/030 | Batch 0300/0391 | Cost: 1.1550\n",
      "Epoch: 007/030 | Batch 0350/0391 | Cost: 0.9117\n",
      "Epoch: 007/030 | Train: 69.412% | Loss: 0.865\n",
      "Time elapsed: 3.55 min\n",
      "Epoch: 008/030 | Batch 0000/0391 | Cost: 0.9418\n",
      "Epoch: 008/030 | Batch 0050/0391 | Cost: 0.7695\n",
      "Epoch: 008/030 | Batch 0100/0391 | Cost: 0.7925\n",
      "Epoch: 008/030 | Batch 0150/0391 | Cost: 1.0822\n",
      "Epoch: 008/030 | Batch 0200/0391 | Cost: 1.1719\n",
      "Epoch: 008/030 | Batch 0250/0391 | Cost: 1.0629\n",
      "Epoch: 008/030 | Batch 0300/0391 | Cost: 0.9214\n",
      "Epoch: 008/030 | Batch 0350/0391 | Cost: 0.9865\n",
      "Epoch: 008/030 | Train: 71.374% | Loss: 0.815\n",
      "Time elapsed: 4.07 min\n",
      "Epoch: 009/030 | Batch 0000/0391 | Cost: 0.7960\n",
      "Epoch: 009/030 | Batch 0050/0391 | Cost: 0.7971\n",
      "Epoch: 009/030 | Batch 0100/0391 | Cost: 0.8975\n",
      "Epoch: 009/030 | Batch 0150/0391 | Cost: 0.8656\n",
      "Epoch: 009/030 | Batch 0200/0391 | Cost: 0.7856\n",
      "Epoch: 009/030 | Batch 0250/0391 | Cost: 0.7853\n",
      "Epoch: 009/030 | Batch 0300/0391 | Cost: 0.8246\n",
      "Epoch: 009/030 | Batch 0350/0391 | Cost: 0.6999\n",
      "Epoch: 009/030 | Train: 75.712% | Loss: 0.702\n",
      "Time elapsed: 4.57 min\n",
      "Epoch: 010/030 | Batch 0000/0391 | Cost: 0.7669\n",
      "Epoch: 010/030 | Batch 0050/0391 | Cost: 0.8405\n",
      "Epoch: 010/030 | Batch 0100/0391 | Cost: 0.7430\n",
      "Epoch: 010/030 | Batch 0150/0391 | Cost: 0.5998\n",
      "Epoch: 010/030 | Batch 0200/0391 | Cost: 0.8150\n",
      "Epoch: 010/030 | Batch 0250/0391 | Cost: 0.6887\n",
      "Epoch: 010/030 | Batch 0300/0391 | Cost: 0.6560\n",
      "Epoch: 010/030 | Batch 0350/0391 | Cost: 0.7469\n",
      "Epoch: 010/030 | Train: 76.626% | Loss: 0.670\n",
      "Time elapsed: 5.09 min\n",
      "Epoch: 011/030 | Batch 0000/0391 | Cost: 0.6520\n",
      "Epoch: 011/030 | Batch 0050/0391 | Cost: 0.7153\n",
      "Epoch: 011/030 | Batch 0100/0391 | Cost: 0.7552\n",
      "Epoch: 011/030 | Batch 0150/0391 | Cost: 0.7801\n",
      "Epoch: 011/030 | Batch 0200/0391 | Cost: 0.6826\n",
      "Epoch: 011/030 | Batch 0250/0391 | Cost: 0.6079\n",
      "Epoch: 011/030 | Batch 0300/0391 | Cost: 0.6860\n",
      "Epoch: 011/030 | Batch 0350/0391 | Cost: 0.6529\n",
      "Epoch: 011/030 | Train: 77.570% | Loss: 0.644\n",
      "Time elapsed: 5.60 min\n",
      "Epoch: 012/030 | Batch 0000/0391 | Cost: 0.4832\n",
      "Epoch: 012/030 | Batch 0050/0391 | Cost: 0.6394\n",
      "Epoch: 012/030 | Batch 0100/0391 | Cost: 0.6139\n",
      "Epoch: 012/030 | Batch 0150/0391 | Cost: 0.6702\n",
      "Epoch: 012/030 | Batch 0200/0391 | Cost: 0.7994\n",
      "Epoch: 012/030 | Batch 0250/0391 | Cost: 0.7631\n",
      "Epoch: 012/030 | Batch 0300/0391 | Cost: 0.5990\n",
      "Epoch: 012/030 | Batch 0350/0391 | Cost: 0.7920\n",
      "Epoch: 012/030 | Train: 80.146% | Loss: 0.575\n",
      "Time elapsed: 6.11 min\n",
      "Epoch: 013/030 | Batch 0000/0391 | Cost: 0.4466\n",
      "Epoch: 013/030 | Batch 0050/0391 | Cost: 0.4625\n",
      "Epoch: 013/030 | Batch 0100/0391 | Cost: 0.5305\n",
      "Epoch: 013/030 | Batch 0150/0391 | Cost: 0.6699\n",
      "Epoch: 013/030 | Batch 0200/0391 | Cost: 0.6843\n",
      "Epoch: 013/030 | Batch 0250/0391 | Cost: 0.5732\n",
      "Epoch: 013/030 | Batch 0300/0391 | Cost: 0.5216\n",
      "Epoch: 013/030 | Batch 0350/0391 | Cost: 0.7692\n",
      "Epoch: 013/030 | Train: 82.234% | Loss: 0.518\n",
      "Time elapsed: 6.62 min\n",
      "Epoch: 014/030 | Batch 0000/0391 | Cost: 0.5080\n",
      "Epoch: 014/030 | Batch 0050/0391 | Cost: 0.4462\n",
      "Epoch: 014/030 | Batch 0100/0391 | Cost: 0.5944\n",
      "Epoch: 014/030 | Batch 0150/0391 | Cost: 0.5441\n",
      "Epoch: 014/030 | Batch 0200/0391 | Cost: 0.7750\n",
      "Epoch: 014/030 | Batch 0250/0391 | Cost: 0.5199\n",
      "Epoch: 014/030 | Batch 0300/0391 | Cost: 0.7396\n",
      "Epoch: 014/030 | Batch 0350/0391 | Cost: 0.5001\n",
      "Epoch: 014/030 | Train: 82.742% | Loss: 0.501\n",
      "Time elapsed: 7.13 min\n",
      "Epoch: 015/030 | Batch 0000/0391 | Cost: 0.4917\n",
      "Epoch: 015/030 | Batch 0050/0391 | Cost: 0.4681\n",
      "Epoch: 015/030 | Batch 0100/0391 | Cost: 0.5142\n",
      "Epoch: 015/030 | Batch 0150/0391 | Cost: 0.5839\n",
      "Epoch: 015/030 | Batch 0200/0391 | Cost: 0.5843\n",
      "Epoch: 015/030 | Batch 0250/0391 | Cost: 0.6265\n",
      "Epoch: 015/030 | Batch 0300/0391 | Cost: 0.6245\n",
      "Epoch: 015/030 | Batch 0350/0391 | Cost: 0.6953\n",
      "Epoch: 015/030 | Train: 85.032% | Loss: 0.453\n",
      "Time elapsed: 7.64 min\n",
      "Epoch: 016/030 | Batch 0000/0391 | Cost: 0.4595\n",
      "Epoch: 016/030 | Batch 0050/0391 | Cost: 0.4455\n",
      "Epoch: 016/030 | Batch 0100/0391 | Cost: 0.5004\n",
      "Epoch: 016/030 | Batch 0150/0391 | Cost: 0.5476\n",
      "Epoch: 016/030 | Batch 0200/0391 | Cost: 0.4570\n",
      "Epoch: 016/030 | Batch 0250/0391 | Cost: 0.4466\n",
      "Epoch: 016/030 | Batch 0300/0391 | Cost: 0.7764\n",
      "Epoch: 016/030 | Batch 0350/0391 | Cost: 0.4904\n",
      "Epoch: 016/030 | Train: 85.720% | Loss: 0.432\n",
      "Time elapsed: 8.16 min\n",
      "Epoch: 017/030 | Batch 0000/0391 | Cost: 0.5702\n",
      "Epoch: 017/030 | Batch 0050/0391 | Cost: 0.3862\n",
      "Epoch: 017/030 | Batch 0100/0391 | Cost: 0.6133\n",
      "Epoch: 017/030 | Batch 0150/0391 | Cost: 0.4691\n",
      "Epoch: 017/030 | Batch 0200/0391 | Cost: 0.5005\n",
      "Epoch: 017/030 | Batch 0250/0391 | Cost: 0.4316\n",
      "Epoch: 017/030 | Batch 0300/0391 | Cost: 0.4979\n",
      "Epoch: 017/030 | Batch 0350/0391 | Cost: 0.5429\n",
      "Epoch: 017/030 | Train: 86.062% | Loss: 0.409\n",
      "Time elapsed: 8.66 min\n",
      "Epoch: 018/030 | Batch 0000/0391 | Cost: 0.3324\n",
      "Epoch: 018/030 | Batch 0050/0391 | Cost: 0.5351\n",
      "Epoch: 018/030 | Batch 0100/0391 | Cost: 0.5819\n",
      "Epoch: 018/030 | Batch 0150/0391 | Cost: 0.4686\n",
      "Epoch: 018/030 | Batch 0200/0391 | Cost: 0.5268\n",
      "Epoch: 018/030 | Batch 0250/0391 | Cost: 0.4752\n",
      "Epoch: 018/030 | Batch 0300/0391 | Cost: 0.4163\n",
      "Epoch: 018/030 | Batch 0350/0391 | Cost: 0.5763\n",
      "Epoch: 018/030 | Train: 86.990% | Loss: 0.392\n",
      "Time elapsed: 9.18 min\n",
      "Epoch: 019/030 | Batch 0000/0391 | Cost: 0.3932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 019/030 | Batch 0050/0391 | Cost: 0.3950\n",
      "Epoch: 019/030 | Batch 0100/0391 | Cost: 0.4053\n",
      "Epoch: 019/030 | Batch 0150/0391 | Cost: 0.5003\n",
      "Epoch: 019/030 | Batch 0200/0391 | Cost: 0.3943\n",
      "Epoch: 019/030 | Batch 0250/0391 | Cost: 0.4935\n",
      "Epoch: 019/030 | Batch 0300/0391 | Cost: 0.5115\n",
      "Epoch: 019/030 | Batch 0350/0391 | Cost: 0.5161\n",
      "Epoch: 019/030 | Train: 87.340% | Loss: 0.375\n",
      "Time elapsed: 9.69 min\n",
      "Epoch: 020/030 | Batch 0000/0391 | Cost: 0.3606\n",
      "Epoch: 020/030 | Batch 0050/0391 | Cost: 0.2998\n",
      "Epoch: 020/030 | Batch 0100/0391 | Cost: 0.3240\n",
      "Epoch: 020/030 | Batch 0150/0391 | Cost: 0.6564\n",
      "Epoch: 020/030 | Batch 0200/0391 | Cost: 0.4182\n",
      "Epoch: 020/030 | Batch 0250/0391 | Cost: 0.5643\n",
      "Epoch: 020/030 | Batch 0300/0391 | Cost: 0.5072\n",
      "Epoch: 020/030 | Batch 0350/0391 | Cost: 0.4762\n",
      "Epoch: 020/030 | Train: 89.174% | Loss: 0.332\n",
      "Time elapsed: 10.21 min\n",
      "Epoch: 021/030 | Batch 0000/0391 | Cost: 0.3365\n",
      "Epoch: 021/030 | Batch 0050/0391 | Cost: 0.3028\n",
      "Epoch: 021/030 | Batch 0100/0391 | Cost: 0.3514\n",
      "Epoch: 021/030 | Batch 0150/0391 | Cost: 0.3499\n",
      "Epoch: 021/030 | Batch 0200/0391 | Cost: 0.4013\n",
      "Epoch: 021/030 | Batch 0250/0391 | Cost: 0.3425\n",
      "Epoch: 021/030 | Batch 0300/0391 | Cost: 0.2918\n",
      "Epoch: 021/030 | Batch 0350/0391 | Cost: 0.4782\n",
      "Epoch: 021/030 | Train: 89.074% | Loss: 0.331\n",
      "Time elapsed: 10.73 min\n",
      "Epoch: 022/030 | Batch 0000/0391 | Cost: 0.3739\n",
      "Epoch: 022/030 | Batch 0050/0391 | Cost: 0.2451\n",
      "Epoch: 022/030 | Batch 0100/0391 | Cost: 0.3657\n",
      "Epoch: 022/030 | Batch 0150/0391 | Cost: 0.4642\n",
      "Epoch: 022/030 | Batch 0200/0391 | Cost: 0.3572\n",
      "Epoch: 022/030 | Batch 0250/0391 | Cost: 0.5540\n",
      "Epoch: 022/030 | Batch 0300/0391 | Cost: 0.3373\n",
      "Epoch: 022/030 | Batch 0350/0391 | Cost: 0.3835\n",
      "Epoch: 022/030 | Train: 89.838% | Loss: 0.311\n",
      "Time elapsed: 11.23 min\n",
      "Epoch: 023/030 | Batch 0000/0391 | Cost: 0.3049\n",
      "Epoch: 023/030 | Batch 0050/0391 | Cost: 0.3411\n",
      "Epoch: 023/030 | Batch 0100/0391 | Cost: 0.3266\n",
      "Epoch: 023/030 | Batch 0150/0391 | Cost: 0.3095\n",
      "Epoch: 023/030 | Batch 0200/0391 | Cost: 0.3265\n",
      "Epoch: 023/030 | Batch 0250/0391 | Cost: 0.3035\n",
      "Epoch: 023/030 | Batch 0300/0391 | Cost: 0.3503\n",
      "Epoch: 023/030 | Batch 0350/0391 | Cost: 0.3072\n",
      "Epoch: 023/030 | Train: 90.816% | Loss: 0.282\n",
      "Time elapsed: 11.74 min\n",
      "Epoch: 024/030 | Batch 0000/0391 | Cost: 0.2019\n",
      "Epoch: 024/030 | Batch 0050/0391 | Cost: 0.4017\n",
      "Epoch: 024/030 | Batch 0100/0391 | Cost: 0.2877\n",
      "Epoch: 024/030 | Batch 0150/0391 | Cost: 0.6416\n",
      "Epoch: 024/030 | Batch 0200/0391 | Cost: 0.4732\n",
      "Epoch: 024/030 | Batch 0250/0391 | Cost: 0.2403\n",
      "Epoch: 024/030 | Batch 0300/0391 | Cost: 0.4354\n",
      "Epoch: 024/030 | Batch 0350/0391 | Cost: 0.5286\n",
      "Epoch: 024/030 | Train: 89.506% | Loss: 0.327\n",
      "Time elapsed: 12.27 min\n",
      "Epoch: 025/030 | Batch 0000/0391 | Cost: 0.2417\n",
      "Epoch: 025/030 | Batch 0050/0391 | Cost: 0.2606\n",
      "Epoch: 025/030 | Batch 0100/0391 | Cost: 0.3345\n",
      "Epoch: 025/030 | Batch 0150/0391 | Cost: 0.3001\n",
      "Epoch: 025/030 | Batch 0200/0391 | Cost: 0.4204\n",
      "Epoch: 025/030 | Batch 0250/0391 | Cost: 0.3004\n",
      "Epoch: 025/030 | Batch 0300/0391 | Cost: 0.2853\n",
      "Epoch: 025/030 | Batch 0350/0391 | Cost: 0.2644\n",
      "Epoch: 025/030 | Train: 91.920% | Loss: 0.258\n",
      "Time elapsed: 12.78 min\n",
      "Epoch: 026/030 | Batch 0000/0391 | Cost: 0.3250\n",
      "Epoch: 026/030 | Batch 0050/0391 | Cost: 0.2770\n",
      "Epoch: 026/030 | Batch 0100/0391 | Cost: 0.2499\n",
      "Epoch: 026/030 | Batch 0150/0391 | Cost: 0.3286\n",
      "Epoch: 026/030 | Batch 0200/0391 | Cost: 0.2636\n",
      "Epoch: 026/030 | Batch 0250/0391 | Cost: 0.3144\n",
      "Epoch: 026/030 | Batch 0300/0391 | Cost: 0.3075\n",
      "Epoch: 026/030 | Batch 0350/0391 | Cost: 0.4395\n",
      "Epoch: 026/030 | Train: 92.540% | Loss: 0.229\n",
      "Time elapsed: 13.29 min\n",
      "Epoch: 027/030 | Batch 0000/0391 | Cost: 0.1689\n",
      "Epoch: 027/030 | Batch 0050/0391 | Cost: 0.2041\n",
      "Epoch: 027/030 | Batch 0100/0391 | Cost: 0.2717\n",
      "Epoch: 027/030 | Batch 0150/0391 | Cost: 0.2265\n",
      "Epoch: 027/030 | Batch 0200/0391 | Cost: 0.3885\n",
      "Epoch: 027/030 | Batch 0250/0391 | Cost: 0.2819\n",
      "Epoch: 027/030 | Batch 0300/0391 | Cost: 0.4921\n",
      "Epoch: 027/030 | Batch 0350/0391 | Cost: 0.2633\n",
      "Epoch: 027/030 | Train: 90.636% | Loss: 0.288\n",
      "Time elapsed: 13.81 min\n",
      "Epoch: 028/030 | Batch 0000/0391 | Cost: 0.2667\n",
      "Epoch: 028/030 | Batch 0050/0391 | Cost: 0.1473\n",
      "Epoch: 028/030 | Batch 0100/0391 | Cost: 0.1687\n",
      "Epoch: 028/030 | Batch 0150/0391 | Cost: 0.1632\n",
      "Epoch: 028/030 | Batch 0200/0391 | Cost: 0.2011\n",
      "Epoch: 028/030 | Batch 0250/0391 | Cost: 0.2205\n",
      "Epoch: 028/030 | Batch 0300/0391 | Cost: 0.3027\n",
      "Epoch: 028/030 | Batch 0350/0391 | Cost: 0.2603\n",
      "Epoch: 028/030 | Train: 92.432% | Loss: 0.232\n",
      "Time elapsed: 14.32 min\n",
      "Epoch: 029/030 | Batch 0000/0391 | Cost: 0.3016\n",
      "Epoch: 029/030 | Batch 0050/0391 | Cost: 0.2970\n",
      "Epoch: 029/030 | Batch 0100/0391 | Cost: 0.1992\n",
      "Epoch: 029/030 | Batch 0150/0391 | Cost: 0.2763\n",
      "Epoch: 029/030 | Batch 0200/0391 | Cost: 0.1862\n",
      "Epoch: 029/030 | Batch 0250/0391 | Cost: 0.1881\n",
      "Epoch: 029/030 | Batch 0300/0391 | Cost: 0.4438\n",
      "Epoch: 029/030 | Batch 0350/0391 | Cost: 0.3535\n",
      "Epoch: 029/030 | Train: 91.664% | Loss: 0.265\n",
      "Time elapsed: 14.84 min\n",
      "Epoch: 030/030 | Batch 0000/0391 | Cost: 0.2146\n",
      "Epoch: 030/030 | Batch 0050/0391 | Cost: 0.3574\n",
      "Epoch: 030/030 | Batch 0100/0391 | Cost: 0.2408\n",
      "Epoch: 030/030 | Batch 0150/0391 | Cost: 0.2562\n",
      "Epoch: 030/030 | Batch 0200/0391 | Cost: 0.2861\n",
      "Epoch: 030/030 | Batch 0250/0391 | Cost: 0.2327\n",
      "Epoch: 030/030 | Batch 0300/0391 | Cost: 0.2559\n",
      "Epoch: 030/030 | Batch 0350/0391 | Cost: 0.2779\n",
      "Epoch: 030/030 | Train: 93.008% | Loss: 0.229\n",
      "Time elapsed: 15.36 min\n",
      "Total Training Time: 15.36 min\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "\n",
    "\n",
    "def compute_epoch_loss(model, data_loader):\n",
    "    model.eval()\n",
    "    curr_loss, num_examples = 0., 0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "            logits, probas = model(features)\n",
    "            loss = F.cross_entropy(logits, targets, reduction='sum')\n",
    "            num_examples += targets.size(0)\n",
    "            curr_loss += loss\n",
    "\n",
    "        curr_loss = curr_loss / num_examples\n",
    "        return curr_loss\n",
    "    \n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        print('Epoch: %03d/%03d | Train: %.3f%% | Loss: %.3f' % (\n",
    "              epoch+1, num_epochs, \n",
    "              compute_accuracy(model, train_loader),\n",
    "              compute_epoch_loss(model, train_loader)))\n",
    "\n",
    "\n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    \n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 76.14%\n"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
