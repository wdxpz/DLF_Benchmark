{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "# Hyperparameters\n",
    "random_seed = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "batch_size = 128\n",
    "\n",
    "# Architecture\n",
    "num_features = 784\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Image batch dimensions: torch.Size([128, 3, 32, 32])\n",
      "Image label dimensions: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### MNIST DATASET\n",
    "##########################\n",
    "\n",
    "# Note transforms.ToTensor() scales input images\n",
    "# to 0-1 range\n",
    "train_dataset = datasets.CIFAR10(root='dataset', \n",
    "                                 train=True, \n",
    "                                 transform=transforms.ToTensor(),\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='dataset', \n",
    "                                train=False, \n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         shuffle=False)\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "\n",
    "class VGG16(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(VGG16, self).__init__()\n",
    "        \n",
    "        # calculate same padding:\n",
    "        # (w - k + 2*p)/s + 1 = o\n",
    "        # => p = (s(o-1) - w + k)/2\n",
    "        \n",
    "        self.block_1 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=3,\n",
    "                          out_channels=64,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          # (1(32-1)- 32 + 3)/2 = 1\n",
    "                          padding=1), \n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=64,\n",
    "                          out_channels=64,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        self.block_2 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=64,\n",
    "                          out_channels=128,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=128,\n",
    "                          out_channels=128,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        self.block_3 = nn.Sequential(        \n",
    "                nn.Conv2d(in_channels=128,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),        \n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "          \n",
    "        self.block_4 = nn.Sequential(   \n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),        \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),        \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),            \n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        self.block_5 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),            \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),            \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),    \n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))             \n",
    "        )\n",
    "            \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 4096),\n",
    "            nn.ReLU(True),\n",
    "            #nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            #nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "            \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.detach().zero_()\n",
    "                    \n",
    "        #self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.block_3(x)\n",
    "        x = self.block_4(x)\n",
    "        x = self.block_5(x)\n",
    "        #x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.classifier(x)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "\n",
    "        return logits, probas\n",
    "\n",
    "    \n",
    "torch.manual_seed(random_seed)\n",
    "model = VGG16(num_features=num_features,\n",
    "              num_classes=num_classes)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/030 | Batch 0000/0391 | Cost: 2.4443\n",
      "Epoch: 001/030 | Batch 0050/0391 | Cost: 2.3005\n",
      "Epoch: 001/030 | Batch 0100/0391 | Cost: 2.1200\n",
      "Epoch: 001/030 | Batch 0150/0391 | Cost: 1.9902\n",
      "Epoch: 001/030 | Batch 0200/0391 | Cost: 1.9986\n",
      "Epoch: 001/030 | Batch 0250/0391 | Cost: 1.8640\n",
      "Epoch: 001/030 | Batch 0300/0391 | Cost: 1.8296\n",
      "Epoch: 001/030 | Batch 0350/0391 | Cost: 1.8453\n",
      "Epoch: 001/030 | Train: 32.538% | Loss: 1.727%\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 002/030 | Batch 0000/0391 | Cost: 1.7896\n",
      "Epoch: 002/030 | Batch 0050/0391 | Cost: 1.6838\n",
      "Epoch: 002/030 | Batch 0100/0391 | Cost: 1.6903\n",
      "Epoch: 002/030 | Batch 0150/0391 | Cost: 1.5592\n",
      "Epoch: 002/030 | Batch 0200/0391 | Cost: 1.4412\n",
      "Epoch: 002/030 | Batch 0250/0391 | Cost: 1.5399\n",
      "Epoch: 002/030 | Batch 0300/0391 | Cost: 1.4301\n",
      "Epoch: 002/030 | Batch 0350/0391 | Cost: 1.3492\n",
      "Epoch: 002/030 | Train: 49.486% | Loss: 1.347%\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 003/030 | Batch 0000/0391 | Cost: 1.4403\n",
      "Epoch: 003/030 | Batch 0050/0391 | Cost: 1.2978\n",
      "Epoch: 003/030 | Batch 0100/0391 | Cost: 1.3559\n",
      "Epoch: 003/030 | Batch 0150/0391 | Cost: 1.1854\n",
      "Epoch: 003/030 | Batch 0200/0391 | Cost: 1.2533\n",
      "Epoch: 003/030 | Batch 0250/0391 | Cost: 1.0098\n",
      "Epoch: 003/030 | Batch 0300/0391 | Cost: 1.1470\n",
      "Epoch: 003/030 | Batch 0350/0391 | Cost: 1.0238\n",
      "Epoch: 003/030 | Train: 63.590% | Loss: 1.020%\n",
      "Time elapsed: 1.58 min\n",
      "Epoch: 004/030 | Batch 0000/0391 | Cost: 1.0967\n",
      "Epoch: 004/030 | Batch 0050/0391 | Cost: 1.0504\n",
      "Epoch: 004/030 | Batch 0100/0391 | Cost: 0.9967\n",
      "Epoch: 004/030 | Batch 0150/0391 | Cost: 0.9057\n",
      "Epoch: 004/030 | Batch 0200/0391 | Cost: 1.0759\n",
      "Epoch: 004/030 | Batch 0250/0391 | Cost: 1.0672\n",
      "Epoch: 004/030 | Batch 0300/0391 | Cost: 0.9357\n",
      "Epoch: 004/030 | Batch 0350/0391 | Cost: 0.8636\n",
      "Epoch: 004/030 | Train: 71.424% | Loss: 0.820%\n",
      "Time elapsed: 2.11 min\n",
      "Epoch: 005/030 | Batch 0000/0391 | Cost: 0.8356\n",
      "Epoch: 005/030 | Batch 0050/0391 | Cost: 0.6962\n",
      "Epoch: 005/030 | Batch 0100/0391 | Cost: 0.9418\n",
      "Epoch: 005/030 | Batch 0150/0391 | Cost: 0.8129\n",
      "Epoch: 005/030 | Batch 0200/0391 | Cost: 0.8085\n",
      "Epoch: 005/030 | Batch 0250/0391 | Cost: 0.6383\n",
      "Epoch: 005/030 | Batch 0300/0391 | Cost: 0.7180\n",
      "Epoch: 005/030 | Batch 0350/0391 | Cost: 0.7066\n",
      "Epoch: 005/030 | Train: 75.948% | Loss: 0.702%\n",
      "Time elapsed: 2.64 min\n",
      "Epoch: 006/030 | Batch 0000/0391 | Cost: 0.6715\n",
      "Epoch: 006/030 | Batch 0050/0391 | Cost: 0.6532\n",
      "Epoch: 006/030 | Batch 0100/0391 | Cost: 0.8586\n",
      "Epoch: 006/030 | Batch 0150/0391 | Cost: 0.6016\n",
      "Epoch: 006/030 | Batch 0200/0391 | Cost: 0.5598\n",
      "Epoch: 006/030 | Batch 0250/0391 | Cost: 0.6490\n",
      "Epoch: 006/030 | Batch 0300/0391 | Cost: 0.8739\n",
      "Epoch: 006/030 | Batch 0350/0391 | Cost: 0.6570\n",
      "Epoch: 006/030 | Train: 82.218% | Loss: 0.541%\n",
      "Time elapsed: 3.18 min\n",
      "Epoch: 007/030 | Batch 0000/0391 | Cost: 0.5127\n",
      "Epoch: 007/030 | Batch 0050/0391 | Cost: 0.4686\n",
      "Epoch: 007/030 | Batch 0100/0391 | Cost: 0.6464\n",
      "Epoch: 007/030 | Batch 0150/0391 | Cost: 0.7649\n",
      "Epoch: 007/030 | Batch 0200/0391 | Cost: 0.5167\n",
      "Epoch: 007/030 | Batch 0250/0391 | Cost: 0.6020\n",
      "Epoch: 007/030 | Batch 0300/0391 | Cost: 0.6707\n",
      "Epoch: 007/030 | Batch 0350/0391 | Cost: 0.6127\n",
      "Epoch: 007/030 | Train: 85.572% | Loss: 0.437%\n",
      "Time elapsed: 3.73 min\n",
      "Epoch: 008/030 | Batch 0000/0391 | Cost: 0.4669\n",
      "Epoch: 008/030 | Batch 0050/0391 | Cost: 0.5261\n",
      "Epoch: 008/030 | Batch 0100/0391 | Cost: 0.4159\n",
      "Epoch: 008/030 | Batch 0150/0391 | Cost: 0.4659\n",
      "Epoch: 008/030 | Batch 0200/0391 | Cost: 0.7307\n",
      "Epoch: 008/030 | Batch 0250/0391 | Cost: 0.5430\n",
      "Epoch: 008/030 | Batch 0300/0391 | Cost: 0.5938\n",
      "Epoch: 008/030 | Batch 0350/0391 | Cost: 0.4640\n",
      "Epoch: 008/030 | Train: 87.498% | Loss: 0.390%\n",
      "Time elapsed: 4.27 min\n",
      "Epoch: 009/030 | Batch 0000/0391 | Cost: 0.5026\n",
      "Epoch: 009/030 | Batch 0050/0391 | Cost: 0.4927\n",
      "Epoch: 009/030 | Batch 0100/0391 | Cost: 0.4077\n",
      "Epoch: 009/030 | Batch 0150/0391 | Cost: 0.4370\n",
      "Epoch: 009/030 | Batch 0200/0391 | Cost: 0.4274\n",
      "Epoch: 009/030 | Batch 0250/0391 | Cost: 0.3612\n",
      "Epoch: 009/030 | Batch 0300/0391 | Cost: 0.4623\n",
      "Epoch: 009/030 | Batch 0350/0391 | Cost: 0.4910\n",
      "Epoch: 009/030 | Train: 86.030% | Loss: 0.398%\n",
      "Time elapsed: 4.82 min\n",
      "Epoch: 010/030 | Batch 0000/0391 | Cost: 0.3435\n",
      "Epoch: 010/030 | Batch 0050/0391 | Cost: 0.3661\n",
      "Epoch: 010/030 | Batch 0100/0391 | Cost: 0.4160\n",
      "Epoch: 010/030 | Batch 0150/0391 | Cost: 0.3357\n",
      "Epoch: 010/030 | Batch 0200/0391 | Cost: 0.3236\n",
      "Epoch: 010/030 | Batch 0250/0391 | Cost: 0.3005\n",
      "Epoch: 010/030 | Batch 0300/0391 | Cost: 0.4624\n",
      "Epoch: 010/030 | Batch 0350/0391 | Cost: 0.4248\n",
      "Epoch: 010/030 | Train: 88.124% | Loss: 0.356%\n",
      "Time elapsed: 5.38 min\n",
      "Epoch: 011/030 | Batch 0000/0391 | Cost: 0.3871\n",
      "Epoch: 011/030 | Batch 0050/0391 | Cost: 0.3412\n",
      "Epoch: 011/030 | Batch 0100/0391 | Cost: 0.3585\n",
      "Epoch: 011/030 | Batch 0150/0391 | Cost: 0.3265\n",
      "Epoch: 011/030 | Batch 0200/0391 | Cost: 0.5328\n",
      "Epoch: 011/030 | Batch 0250/0391 | Cost: 0.3792\n",
      "Epoch: 011/030 | Batch 0300/0391 | Cost: 0.4439\n",
      "Epoch: 011/030 | Batch 0350/0391 | Cost: 0.3251\n",
      "Epoch: 011/030 | Train: 90.672% | Loss: 0.290%\n",
      "Time elapsed: 5.94 min\n",
      "Epoch: 012/030 | Batch 0000/0391 | Cost: 0.2277\n",
      "Epoch: 012/030 | Batch 0050/0391 | Cost: 0.3048\n",
      "Epoch: 012/030 | Batch 0100/0391 | Cost: 0.2728\n",
      "Epoch: 012/030 | Batch 0150/0391 | Cost: 0.2344\n",
      "Epoch: 012/030 | Batch 0200/0391 | Cost: 0.3180\n",
      "Epoch: 012/030 | Batch 0250/0391 | Cost: 0.3186\n",
      "Epoch: 012/030 | Batch 0300/0391 | Cost: 0.3033\n",
      "Epoch: 012/030 | Batch 0350/0391 | Cost: 0.3329\n",
      "Epoch: 012/030 | Train: 93.856% | Loss: 0.197%\n",
      "Time elapsed: 6.48 min\n",
      "Epoch: 013/030 | Batch 0000/0391 | Cost: 0.2546\n",
      "Epoch: 013/030 | Batch 0050/0391 | Cost: 0.2241\n",
      "Epoch: 013/030 | Batch 0100/0391 | Cost: 0.2945\n",
      "Epoch: 013/030 | Batch 0150/0391 | Cost: 0.2467\n",
      "Epoch: 013/030 | Batch 0200/0391 | Cost: 0.1465\n",
      "Epoch: 013/030 | Batch 0250/0391 | Cost: 0.2390\n",
      "Epoch: 013/030 | Batch 0300/0391 | Cost: 0.2218\n",
      "Epoch: 013/030 | Batch 0350/0391 | Cost: 0.3233\n",
      "Epoch: 013/030 | Train: 94.220% | Loss: 0.185%\n",
      "Time elapsed: 7.03 min\n",
      "Epoch: 014/030 | Batch 0000/0391 | Cost: 0.2184\n",
      "Epoch: 014/030 | Batch 0050/0391 | Cost: 0.2226\n",
      "Epoch: 014/030 | Batch 0100/0391 | Cost: 0.1892\n",
      "Epoch: 014/030 | Batch 0150/0391 | Cost: 0.2402\n",
      "Epoch: 014/030 | Batch 0200/0391 | Cost: 0.4162\n",
      "Epoch: 014/030 | Batch 0250/0391 | Cost: 0.2230\n",
      "Epoch: 014/030 | Batch 0300/0391 | Cost: 0.4601\n",
      "Epoch: 014/030 | Batch 0350/0391 | Cost: 0.3195\n",
      "Epoch: 014/030 | Train: 93.622% | Loss: 0.198%\n",
      "Time elapsed: 7.58 min\n",
      "Epoch: 015/030 | Batch 0000/0391 | Cost: 0.1609\n",
      "Epoch: 015/030 | Batch 0050/0391 | Cost: 0.1883\n",
      "Epoch: 015/030 | Batch 0100/0391 | Cost: 0.1384\n",
      "Epoch: 015/030 | Batch 0150/0391 | Cost: 0.4214\n",
      "Epoch: 015/030 | Batch 0200/0391 | Cost: 0.2803\n",
      "Epoch: 015/030 | Batch 0250/0391 | Cost: 0.1664\n",
      "Epoch: 015/030 | Batch 0300/0391 | Cost: 0.2037\n",
      "Epoch: 015/030 | Batch 0350/0391 | Cost: 0.1178\n",
      "Epoch: 015/030 | Train: 95.210% | Loss: 0.154%\n",
      "Time elapsed: 8.15 min\n",
      "Epoch: 016/030 | Batch 0000/0391 | Cost: 0.1225\n",
      "Epoch: 016/030 | Batch 0050/0391 | Cost: 0.1933\n",
      "Epoch: 016/030 | Batch 0100/0391 | Cost: 0.0586\n",
      "Epoch: 016/030 | Batch 0150/0391 | Cost: 0.1150\n",
      "Epoch: 016/030 | Batch 0200/0391 | Cost: 0.2771\n",
      "Epoch: 016/030 | Batch 0250/0391 | Cost: 0.3209\n",
      "Epoch: 016/030 | Batch 0300/0391 | Cost: 0.2049\n",
      "Epoch: 016/030 | Batch 0350/0391 | Cost: 0.3282\n",
      "Epoch: 016/030 | Train: 95.150% | Loss: 0.153%\n",
      "Time elapsed: 8.70 min\n",
      "Epoch: 017/030 | Batch 0000/0391 | Cost: 0.0804\n",
      "Epoch: 017/030 | Batch 0050/0391 | Cost: 0.1896\n",
      "Epoch: 017/030 | Batch 0100/0391 | Cost: 0.1776\n",
      "Epoch: 017/030 | Batch 0150/0391 | Cost: 0.1946\n",
      "Epoch: 017/030 | Batch 0200/0391 | Cost: 0.0812\n",
      "Epoch: 017/030 | Batch 0250/0391 | Cost: 0.1365\n",
      "Epoch: 017/030 | Batch 0300/0391 | Cost: 0.2943\n",
      "Epoch: 017/030 | Batch 0350/0391 | Cost: 0.1520\n",
      "Epoch: 017/030 | Train: 93.038% | Loss: 0.242%\n",
      "Time elapsed: 9.25 min\n",
      "Epoch: 018/030 | Batch 0000/0391 | Cost: 0.3275\n",
      "Epoch: 018/030 | Batch 0050/0391 | Cost: 0.1447\n",
      "Epoch: 018/030 | Batch 0100/0391 | Cost: 0.0777\n",
      "Epoch: 018/030 | Batch 0150/0391 | Cost: 0.2188\n",
      "Epoch: 018/030 | Batch 0200/0391 | Cost: 0.2257\n",
      "Epoch: 018/030 | Batch 0250/0391 | Cost: 0.1994\n",
      "Epoch: 018/030 | Batch 0300/0391 | Cost: 0.1205\n",
      "Epoch: 018/030 | Batch 0350/0391 | Cost: 0.1219\n",
      "Epoch: 018/030 | Train: 96.236% | Loss: 0.120%\n",
      "Time elapsed: 9.80 min\n",
      "Epoch: 019/030 | Batch 0000/0391 | Cost: 0.1131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 019/030 | Batch 0050/0391 | Cost: 0.2586\n",
      "Epoch: 019/030 | Batch 0100/0391 | Cost: 0.0585\n",
      "Epoch: 019/030 | Batch 0150/0391 | Cost: 0.3601\n",
      "Epoch: 019/030 | Batch 0200/0391 | Cost: 0.1455\n",
      "Epoch: 019/030 | Batch 0250/0391 | Cost: 0.1897\n",
      "Epoch: 019/030 | Batch 0300/0391 | Cost: 0.1214\n",
      "Epoch: 019/030 | Batch 0350/0391 | Cost: 0.2604\n",
      "Epoch: 019/030 | Train: 95.992% | Loss: 0.134%\n",
      "Time elapsed: 10.39 min\n",
      "Epoch: 020/030 | Batch 0000/0391 | Cost: 0.0584\n",
      "Epoch: 020/030 | Batch 0050/0391 | Cost: 0.1569\n",
      "Epoch: 020/030 | Batch 0100/0391 | Cost: 0.0891\n",
      "Epoch: 020/030 | Batch 0150/0391 | Cost: 0.2129\n",
      "Epoch: 020/030 | Batch 0200/0391 | Cost: 0.2103\n",
      "Epoch: 020/030 | Batch 0250/0391 | Cost: 0.1555\n",
      "Epoch: 020/030 | Batch 0300/0391 | Cost: 0.2779\n",
      "Epoch: 020/030 | Batch 0350/0391 | Cost: 0.1834\n",
      "Epoch: 020/030 | Train: 97.430% | Loss: 0.089%\n",
      "Time elapsed: 10.94 min\n",
      "Epoch: 021/030 | Batch 0000/0391 | Cost: 0.0748\n",
      "Epoch: 021/030 | Batch 0050/0391 | Cost: 0.1013\n",
      "Epoch: 021/030 | Batch 0100/0391 | Cost: 0.2556\n",
      "Epoch: 021/030 | Batch 0150/0391 | Cost: 0.1727\n",
      "Epoch: 021/030 | Batch 0200/0391 | Cost: 0.0471\n",
      "Epoch: 021/030 | Batch 0250/0391 | Cost: 0.1021\n",
      "Epoch: 021/030 | Batch 0300/0391 | Cost: 0.1565\n",
      "Epoch: 021/030 | Batch 0350/0391 | Cost: 0.2030\n",
      "Epoch: 021/030 | Train: 96.614% | Loss: 0.112%\n",
      "Time elapsed: 11.49 min\n",
      "Epoch: 022/030 | Batch 0000/0391 | Cost: 0.0803\n",
      "Epoch: 022/030 | Batch 0050/0391 | Cost: 0.1049\n",
      "Epoch: 022/030 | Batch 0100/0391 | Cost: 0.1728\n",
      "Epoch: 022/030 | Batch 0150/0391 | Cost: 0.2532\n",
      "Epoch: 022/030 | Batch 0200/0391 | Cost: 0.1277\n",
      "Epoch: 022/030 | Batch 0250/0391 | Cost: 0.2338\n",
      "Epoch: 022/030 | Batch 0300/0391 | Cost: 0.2284\n",
      "Epoch: 022/030 | Batch 0350/0391 | Cost: 0.1370\n",
      "Epoch: 022/030 | Train: 97.274% | Loss: 0.090%\n",
      "Time elapsed: 12.04 min\n",
      "Epoch: 023/030 | Batch 0000/0391 | Cost: 0.1821\n",
      "Epoch: 023/030 | Batch 0050/0391 | Cost: 0.0599\n",
      "Epoch: 023/030 | Batch 0100/0391 | Cost: 0.0890\n",
      "Epoch: 023/030 | Batch 0150/0391 | Cost: 0.1502\n",
      "Epoch: 023/030 | Batch 0200/0391 | Cost: 0.0591\n",
      "Epoch: 023/030 | Batch 0250/0391 | Cost: 0.0211\n",
      "Epoch: 023/030 | Batch 0300/0391 | Cost: 0.0978\n",
      "Epoch: 023/030 | Batch 0350/0391 | Cost: 0.1780\n",
      "Epoch: 023/030 | Train: 96.570% | Loss: 0.113%\n",
      "Time elapsed: 12.64 min\n",
      "Epoch: 024/030 | Batch 0000/0391 | Cost: 0.1444\n",
      "Epoch: 024/030 | Batch 0050/0391 | Cost: 0.0565\n",
      "Epoch: 024/030 | Batch 0100/0391 | Cost: 0.0238\n",
      "Epoch: 024/030 | Batch 0150/0391 | Cost: 0.0557\n",
      "Epoch: 024/030 | Batch 0200/0391 | Cost: 0.1138\n",
      "Epoch: 024/030 | Batch 0250/0391 | Cost: 0.1179\n",
      "Epoch: 024/030 | Batch 0300/0391 | Cost: 0.0396\n",
      "Epoch: 024/030 | Batch 0350/0391 | Cost: 0.1814\n",
      "Epoch: 024/030 | Train: 97.644% | Loss: 0.080%\n",
      "Time elapsed: 13.20 min\n",
      "Epoch: 025/030 | Batch 0000/0391 | Cost: 0.0648\n",
      "Epoch: 025/030 | Batch 0050/0391 | Cost: 0.1722\n",
      "Epoch: 025/030 | Batch 0100/0391 | Cost: 0.0828\n",
      "Epoch: 025/030 | Batch 0150/0391 | Cost: 0.0440\n",
      "Epoch: 025/030 | Batch 0200/0391 | Cost: 0.1082\n",
      "Epoch: 025/030 | Batch 0250/0391 | Cost: 0.1243\n",
      "Epoch: 025/030 | Batch 0300/0391 | Cost: 0.0739\n",
      "Epoch: 025/030 | Batch 0350/0391 | Cost: 0.0699\n",
      "Epoch: 025/030 | Train: 97.302% | Loss: 0.094%\n",
      "Time elapsed: 13.75 min\n",
      "Epoch: 026/030 | Batch 0000/0391 | Cost: 0.0506\n",
      "Epoch: 026/030 | Batch 0050/0391 | Cost: 0.2225\n",
      "Epoch: 026/030 | Batch 0100/0391 | Cost: 0.0885\n",
      "Epoch: 026/030 | Batch 0150/0391 | Cost: 0.1016\n",
      "Epoch: 026/030 | Batch 0200/0391 | Cost: 0.2134\n",
      "Epoch: 026/030 | Batch 0250/0391 | Cost: 0.0773\n",
      "Epoch: 026/030 | Batch 0300/0391 | Cost: 0.2937\n",
      "Epoch: 026/030 | Batch 0350/0391 | Cost: 0.1051\n",
      "Epoch: 026/030 | Train: 97.046% | Loss: 0.104%\n",
      "Time elapsed: 14.30 min\n",
      "Epoch: 027/030 | Batch 0000/0391 | Cost: 0.1080\n",
      "Epoch: 027/030 | Batch 0050/0391 | Cost: 0.1430\n",
      "Epoch: 027/030 | Batch 0100/0391 | Cost: 0.1235\n",
      "Epoch: 027/030 | Batch 0150/0391 | Cost: 0.0399\n",
      "Epoch: 027/030 | Batch 0200/0391 | Cost: 0.0371\n",
      "Epoch: 027/030 | Batch 0250/0391 | Cost: 0.0518\n",
      "Epoch: 027/030 | Batch 0300/0391 | Cost: 0.1292\n",
      "Epoch: 027/030 | Batch 0350/0391 | Cost: 0.2222\n",
      "Epoch: 027/030 | Train: 97.838% | Loss: 0.072%\n",
      "Time elapsed: 14.90 min\n",
      "Epoch: 028/030 | Batch 0000/0391 | Cost: 0.0345\n",
      "Epoch: 028/030 | Batch 0050/0391 | Cost: 0.1362\n",
      "Epoch: 028/030 | Batch 0100/0391 | Cost: 0.1005\n",
      "Epoch: 028/030 | Batch 0150/0391 | Cost: 0.1183\n",
      "Epoch: 028/030 | Batch 0200/0391 | Cost: 0.1542\n",
      "Epoch: 028/030 | Batch 0250/0391 | Cost: 0.1749\n",
      "Epoch: 028/030 | Batch 0300/0391 | Cost: 0.0533\n",
      "Epoch: 028/030 | Batch 0350/0391 | Cost: 0.0848\n",
      "Epoch: 028/030 | Train: 98.014% | Loss: 0.068%\n",
      "Time elapsed: 15.45 min\n",
      "Epoch: 029/030 | Batch 0000/0391 | Cost: 0.0386\n",
      "Epoch: 029/030 | Batch 0050/0391 | Cost: 0.0724\n",
      "Epoch: 029/030 | Batch 0100/0391 | Cost: 0.0423\n",
      "Epoch: 029/030 | Batch 0150/0391 | Cost: 0.0971\n",
      "Epoch: 029/030 | Batch 0200/0391 | Cost: 0.1598\n",
      "Epoch: 029/030 | Batch 0250/0391 | Cost: 0.2033\n",
      "Epoch: 029/030 | Batch 0300/0391 | Cost: 0.0729\n",
      "Epoch: 029/030 | Batch 0350/0391 | Cost: 0.6103\n",
      "Epoch: 029/030 | Train: 92.728% | Loss: 0.235%\n",
      "Time elapsed: 16.00 min\n",
      "Epoch: 030/030 | Batch 0000/0391 | Cost: 0.2197\n",
      "Epoch: 030/030 | Batch 0050/0391 | Cost: 0.1955\n",
      "Epoch: 030/030 | Batch 0100/0391 | Cost: 0.1162\n",
      "Epoch: 030/030 | Batch 0150/0391 | Cost: 0.0893\n",
      "Epoch: 030/030 | Batch 0200/0391 | Cost: 0.1219\n",
      "Epoch: 030/030 | Batch 0250/0391 | Cost: 0.0561\n",
      "Epoch: 030/030 | Batch 0300/0391 | Cost: 0.0878\n",
      "Epoch: 030/030 | Batch 0350/0391 | Cost: 0.1916\n",
      "Epoch: 030/030 | Train: 97.790% | Loss: 0.072%\n",
      "Time elapsed: 16.55 min\n",
      "Total Training Time: 16.55 min\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "\n",
    "\n",
    "def compute_epoch_loss(model, data_loader):\n",
    "    model.eval()\n",
    "    curr_loss, num_examples = 0., 0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "            logits, probas = model(features)\n",
    "            loss = F.cross_entropy(logits, targets, reduction='sum')\n",
    "            num_examples += targets.size(0)\n",
    "            curr_loss += loss\n",
    "\n",
    "        curr_loss = curr_loss / num_examples\n",
    "        return curr_loss\n",
    "    \n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        print('Epoch: %03d/%03d | Train: %.3f%% | Loss: %.3f%%' % (\n",
    "              epoch+1, num_epochs, \n",
    "              compute_accuracy(model, train_loader),\n",
    "              compute_epoch_loss(model, train_loader)))\n",
    "\n",
    "\n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    \n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 79.14%\n"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
